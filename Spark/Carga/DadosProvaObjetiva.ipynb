{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31e718b-14d9-4751-9dec-71f56129a085",
   "metadata": {},
   "source": [
    "## Importação das Bibiliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc20c5a9-44d0-4ee3-ac2c-0f588099a1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pyodbc\n",
    "import pyspark.sql.functions as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92050d4",
   "metadata": {},
   "source": [
    "## Criando uma SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "137c301b-7b56-434b-bd08-979c6717af70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"LerDadosEnem\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ed9e6-77de-42fb-96ed-d1b23d0251a4",
   "metadata": {},
   "source": [
    "## Processo de Leitura do CSV de Participantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "158e76e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Path dos arquivos\n",
    "\n",
    "caminho_arquivo_2022 = r'E:\\Estudos\\SQL\\Datasets\\ENEM\\2022\\microdados_enem_2022\\DADOS\\MICRODADOS_ENEM_2022.csv'\n",
    "caminho_arquivo_2021 = r'E:\\Estudos\\SQL\\Datasets\\ENEM\\2021\\microdados_enem_2021\\DADOS\\MICRODADOS_ENEM_2021.csv'\n",
    "caminho_arquivo_2020 = r'E:\\Estudos\\SQL\\Datasets\\ENEM\\2020\\microdados_enem_2020\\DADOS\\MICRODADOS_ENEM_2020.csv'\n",
    "caminho_arquivo_2019 = r'E:\\Estudos\\SQL\\Datasets\\ENEM\\2019\\microdados_enem_2019\\DADOS\\MICRODADOS_ENEM_2019.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34563783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função Paa ler os arquivos CSV\n",
    "def lerCSV(spark, caminho_arquivo):\n",
    "    df = spark.read.csv(caminho_arquivo, sep=';', header=True, inferSchema=True, encoding='ISO-8859-1')\n",
    "    colunas_selecionadas = [F.col(df.columns[0])] + [F.col(df.columns[i]) for i in range(23, 44)]\n",
    "    return df.select(colunas_selecionadas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc39787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Chama função quer ler o arquivo CSV e Transforma para DF.SPARK\n",
    "\n",
    "dfDadosProvas22 = lerCSV(spark, caminho_arquivo_2022)\n",
    "dfDadosProvas21 = lerCSV(spark, caminho_arquivo_2021)\n",
    "dfDadosProvas20 = lerCSV(spark, caminho_arquivo_2020)\n",
    "dfDadosProvas19 = lerCSV(spark, caminho_arquivo_2019)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e05951bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unir os DF em um só \n",
    "\n",
    "dfDadosProvas = dfDadosProvas22.union(dfDadosProvas21).union(dfDadosProvas20).union(dfDadosProvas19)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b889869-4d51-4802-9259-f27d6fb5db0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+--------------+--------------+--------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|NU_INSCRICAO|TP_PRESENCA_CN|TP_PRESENCA_CH|TP_PRESENCA_LC|TP_PRESENCA_MT|CO_PROVA_CN|CO_PROVA_CH|CO_PROVA_LC|CO_PROVA_MT|NU_NOTA_CN|NU_NOTA_CH|NU_NOTA_LC|NU_NOTA_MT|     TX_RESPOSTAS_CN|     TX_RESPOSTAS_CH|     TX_RESPOSTAS_LC|     TX_RESPOSTAS_MT|TP_LINGUA|      TX_GABARITO_CN|      TX_GABARITO_CH|      TX_GABARITO_LC|      TX_GABARITO_MT|\n",
      "+------------+--------------+--------------+--------------+--------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|210057943671|             0|             0|             0|             0|       null|       null|       null|       null|      null|      null|      null|      null|                null|                null|                null|                null|        0|                null|                null|                null|                null|\n",
      "|210057516120|             0|             0|             0|             0|       null|       null|       null|       null|      null|      null|      null|      null|                null|                null|                null|                null|        0|                null|                null|                null|                null|\n",
      "|210057280536|             1|             1|             1|             1|       1087|       1056|       1066|       1078|     421.1|     546.0|     498.8|     565.3|DCDCEBADDABEDBCBE...|DBDABCADADECACBDC...|ABEABADCCADCAAAAC...|BDEBACECBCAEBABED...|        1|DDECDBEACCAEBEAEB...|ECBABCDDAAECBCBEC...|DECDBDCEAADDBCABE...|BEEDAEABDDCEBDBAA...|\n",
      "|210055724397|             1|             1|             1|             1|       1088|       1058|       1067|       1077|     490.7|     388.6|     357.8|     416.0|BACBBABBCAABCABBC...|BBCAACACDCABCBACC...|ABACCCCBCBAADABBB...|BBBACCBABABBBBACD...|        1|DDCBBCCDDAADBAABE...|CEDAAADBAECBCDDAD...|CDBEDACDEACAABDBD...|CCCCBXCBABECBEABD...|\n",
      "|210055097896|             0|             0|             0|             0|       null|       null|       null|       null|      null|      null|      null|      null|                null|                null|                null|                null|        0|                null|                null|                null|                null|\n",
      "|210057850231|             0|             0|             0|             0|       null|       null|       null|       null|      null|      null|      null|      null|                null|                null|                null|                null|        1|                null|                null|                null|                null|\n",
      "|210056305481|             0|             0|             0|             0|       null|       null|       null|       null|      null|      null|      null|      null|                null|                null|                null|                null|        0|                null|                null|                null|                null|\n",
      "|210055778089|             1|             1|             1|             1|       1086|       1055|       1065|       1076|     398.1|     427.3|     400.2|     404.9|CBCCDBCACACABBDAE...|BABACCABCBADEABDD...|DACECBAEAABBEAACD...|DBCCABCCBCEEBCECB...|        1|EACDDEBEACCCDABCE...|DEABEEDAEBAAECBEC...|DCBDEEDCAAEAAACAB...|DBAAACEBEDAECCBEC...|\n",
      "|210057252239|             0|             0|             0|             0|       null|       null|       null|       null|      null|      null|      null|      null|                null|                null|                null|                null|        1|                null|                null|                null|                null|\n",
      "|210055461467|             1|             1|             1|             1|       1087|       1056|       1066|       1078|     467.5|     461.0|     466.7|     435.3|ACEAACADAEBADDCCA...|EAAABEADCDDCEAACD...|ECEABCBECEDACADAB...|BEBCCCABABCCDCCCB...|        1|DDECDBEACCAEBEAEB...|ECBABCDDAAECBCBEC...|DECDBDCEAADDBCABE...|BEEDAEABDDCEBDBAA...|\n",
      "|210055665589|             0|             0|             0|             0|       null|       null|       null|       null|      null|      null|      null|      null|                null|                null|                null|                null|        1|                null|                null|                null|                null|\n",
      "|210054972570|             1|             1|             1|             1|       1086|       1055|       1065|       1076|     458.7|     539.8|     488.2|     456.8|DDDDDBCDDDBDDADDD...|AEAADEDEDDADDDDED...|EABAAEAAAAAAAAEAA...|DDDDDCDEEDDCCDDDD...|        1|EACDDEBEACCCDABCE...|DEABEEDAEBAAECBEC...|DCBDEEDCAAEAAACAB...|DBAAACEBEDAECCBEC...|\n",
      "|210055449344|             0|             0|             0|             0|       null|       null|       null|       null|      null|      null|      null|      null|                null|                null|                null|                null|        1|                null|                null|                null|                null|\n",
      "|210057944037|             0|             0|             0|             0|       null|       null|       null|       null|      null|      null|      null|      null|                null|                null|                null|                null|        1|                null|                null|                null|                null|\n",
      "|210054861834|             0|             0|             0|             0|       null|       null|       null|       null|      null|      null|      null|      null|                null|                null|                null|                null|        1|                null|                null|                null|                null|\n",
      "|210055201589|             0|             0|             0|             0|       null|       null|       null|       null|      null|      null|      null|      null|                null|                null|                null|                null|        1|                null|                null|                null|                null|\n",
      "|210056844882|             0|             0|             0|             0|       null|       null|       null|       null|      null|      null|      null|      null|                null|                null|                null|                null|        0|                null|                null|                null|                null|\n",
      "|210056065100|             1|             1|             1|             1|       1086|       1055|       1065|       1076|     396.8|     528.7|     551.4|     536.0|ABEACDCACEADBDBDB...|CEABADCEDEBAECBAD...|CCBDBECEAECBDDECA...|CABACCAEEAABEDCCC...|        0|EACDDEBEACCCDABCE...|DEABEEDAEBAAECBEC...|DCBDEEDCAAEAAACAB...|DBAAACEBEDAECCBEC...|\n",
      "|210057551883|             1|             1|             1|             1|       1088|       1058|       1067|       1077|     481.4|     603.6|     589.0|     695.0|DDCDACAEDAEDDBBDA...|DBDAAADEDEBBCDEAD...|ACDEACEBBDBEBEAAA...|CEBBBECAACBBBEAAD...|        1|DDCBBCCDDAADBAABE...|CEDAAADBAECBCDDAD...|CDBEDACDEACAABDBD...|CCCCBXCBABECBEABD...|\n",
      "|210054722657|             0|             0|             0|             0|       null|       null|       null|       null|      null|      null|      null|      null|                null|                null|                null|                null|        0|                null|                null|                null|                null|\n",
      "+------------+--------------+--------------+--------------+--------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDadosProvas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0ddb3df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfPreencher = dfDadosProvas.fillna(-2)  # Preenche com -2(sem ref), por exemplo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ef05a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+--------------+--------------+--------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|NU_INSCRICAO|TP_PRESENCA_CN|TP_PRESENCA_CH|TP_PRESENCA_LC|TP_PRESENCA_MT|CO_PROVA_CN|CO_PROVA_CH|CO_PROVA_LC|CO_PROVA_MT|NU_NOTA_CN|NU_NOTA_CH|NU_NOTA_LC|NU_NOTA_MT|     TX_RESPOSTAS_CN|     TX_RESPOSTAS_CH|     TX_RESPOSTAS_LC|     TX_RESPOSTAS_MT|TP_LINGUA|      TX_GABARITO_CN|      TX_GABARITO_CH|      TX_GABARITO_LC|      TX_GABARITO_MT|\n",
      "+------------+--------------+--------------+--------------+--------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|210057943671|             0|             0|             0|             0|         -2|         -2|         -2|         -2|      -2.0|      -2.0|      -2.0|      -2.0|                null|                null|                null|                null|        0|                null|                null|                null|                null|\n",
      "|210057516120|             0|             0|             0|             0|         -2|         -2|         -2|         -2|      -2.0|      -2.0|      -2.0|      -2.0|                null|                null|                null|                null|        0|                null|                null|                null|                null|\n",
      "|210057280536|             1|             1|             1|             1|       1087|       1056|       1066|       1078|     421.1|     546.0|     498.8|     565.3|DCDCEBADDABEDBCBE...|DBDABCADADECACBDC...|ABEABADCCADCAAAAC...|BDEBACECBCAEBABED...|        1|DDECDBEACCAEBEAEB...|ECBABCDDAAECBCBEC...|DECDBDCEAADDBCABE...|BEEDAEABDDCEBDBAA...|\n",
      "|210055724397|             1|             1|             1|             1|       1088|       1058|       1067|       1077|     490.7|     388.6|     357.8|     416.0|BACBBABBCAABCABBC...|BBCAACACDCABCBACC...|ABACCCCBCBAADABBB...|BBBACCBABABBBBACD...|        1|DDCBBCCDDAADBAABE...|CEDAAADBAECBCDDAD...|CDBEDACDEACAABDBD...|CCCCBXCBABECBEABD...|\n",
      "|210055097896|             0|             0|             0|             0|         -2|         -2|         -2|         -2|      -2.0|      -2.0|      -2.0|      -2.0|                null|                null|                null|                null|        0|                null|                null|                null|                null|\n",
      "|210057850231|             0|             0|             0|             0|         -2|         -2|         -2|         -2|      -2.0|      -2.0|      -2.0|      -2.0|                null|                null|                null|                null|        1|                null|                null|                null|                null|\n",
      "|210056305481|             0|             0|             0|             0|         -2|         -2|         -2|         -2|      -2.0|      -2.0|      -2.0|      -2.0|                null|                null|                null|                null|        0|                null|                null|                null|                null|\n",
      "|210055778089|             1|             1|             1|             1|       1086|       1055|       1065|       1076|     398.1|     427.3|     400.2|     404.9|CBCCDBCACACABBDAE...|BABACCABCBADEABDD...|DACECBAEAABBEAACD...|DBCCABCCBCEEBCECB...|        1|EACDDEBEACCCDABCE...|DEABEEDAEBAAECBEC...|DCBDEEDCAAEAAACAB...|DBAAACEBEDAECCBEC...|\n",
      "|210057252239|             0|             0|             0|             0|         -2|         -2|         -2|         -2|      -2.0|      -2.0|      -2.0|      -2.0|                null|                null|                null|                null|        1|                null|                null|                null|                null|\n",
      "|210055461467|             1|             1|             1|             1|       1087|       1056|       1066|       1078|     467.5|     461.0|     466.7|     435.3|ACEAACADAEBADDCCA...|EAAABEADCDDCEAACD...|ECEABCBECEDACADAB...|BEBCCCABABCCDCCCB...|        1|DDECDBEACCAEBEAEB...|ECBABCDDAAECBCBEC...|DECDBDCEAADDBCABE...|BEEDAEABDDCEBDBAA...|\n",
      "|210055665589|             0|             0|             0|             0|         -2|         -2|         -2|         -2|      -2.0|      -2.0|      -2.0|      -2.0|                null|                null|                null|                null|        1|                null|                null|                null|                null|\n",
      "|210054972570|             1|             1|             1|             1|       1086|       1055|       1065|       1076|     458.7|     539.8|     488.2|     456.8|DDDDDBCDDDBDDADDD...|AEAADEDEDDADDDDED...|EABAAEAAAAAAAAEAA...|DDDDDCDEEDDCCDDDD...|        1|EACDDEBEACCCDABCE...|DEABEEDAEBAAECBEC...|DCBDEEDCAAEAAACAB...|DBAAACEBEDAECCBEC...|\n",
      "|210055449344|             0|             0|             0|             0|         -2|         -2|         -2|         -2|      -2.0|      -2.0|      -2.0|      -2.0|                null|                null|                null|                null|        1|                null|                null|                null|                null|\n",
      "|210057944037|             0|             0|             0|             0|         -2|         -2|         -2|         -2|      -2.0|      -2.0|      -2.0|      -2.0|                null|                null|                null|                null|        1|                null|                null|                null|                null|\n",
      "|210054861834|             0|             0|             0|             0|         -2|         -2|         -2|         -2|      -2.0|      -2.0|      -2.0|      -2.0|                null|                null|                null|                null|        1|                null|                null|                null|                null|\n",
      "|210055201589|             0|             0|             0|             0|         -2|         -2|         -2|         -2|      -2.0|      -2.0|      -2.0|      -2.0|                null|                null|                null|                null|        1|                null|                null|                null|                null|\n",
      "|210056844882|             0|             0|             0|             0|         -2|         -2|         -2|         -2|      -2.0|      -2.0|      -2.0|      -2.0|                null|                null|                null|                null|        0|                null|                null|                null|                null|\n",
      "|210056065100|             1|             1|             1|             1|       1086|       1055|       1065|       1076|     396.8|     528.7|     551.4|     536.0|ABEACDCACEADBDBDB...|CEABADCEDEBAECBAD...|CCBDBECEAECBDDECA...|CABACCAEEAABEDCCC...|        0|EACDDEBEACCCDABCE...|DEABEEDAEBAAECBEC...|DCBDEEDCAAEAAACAB...|DBAAACEBEDAECCBEC...|\n",
      "|210057551883|             1|             1|             1|             1|       1088|       1058|       1067|       1077|     481.4|     603.6|     589.0|     695.0|DDCDACAEDAEDDBBDA...|DBDAAADEDEBBCDEAD...|ACDEACEBBDBEBEAAA...|CEBBBECAACBBBEAAD...|        1|DDCBBCCDDAADBAABE...|CEDAAADBAECBCDDAD...|CDBEDACDEACAABDBD...|CCCCBXCBABECBEABD...|\n",
      "|210054722657|             0|             0|             0|             0|         -2|         -2|         -2|         -2|      -2.0|      -2.0|      -2.0|      -2.0|                null|                null|                null|                null|        0|                null|                null|                null|                null|\n",
      "+------------+--------------+--------------+--------------+--------------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+--------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPreencher.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ed5ca",
   "metadata": {},
   "source": [
    "# Salvar em SQL \n",
    "Salvar os dados em uma tabela SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1deb1f",
   "metadata": {},
   "source": [
    "## Parametrizando a conexão do Banco SQL SERVER Usando JDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e7396db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jdbc:sqlserver://NOTEBOOK-LEO:1433;databaseName=Enem;\n"
     ]
    }
   ],
   "source": [
    "server_name = \"jdbc:sqlserver://NOTEBOOK-LEO:1433\"\n",
    "database_name = \"Enem\"\n",
    "table_name = \"ProvasObjetivas\"\n",
    "username = \"bi_leonardobo\"\n",
    "password = \"0000\"\n",
    "url = server_name + \";\" + \"databaseName=\" + database_name + \";\"\n",
    "print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45262d43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nome da nova tabela que será criada\n",
    "nomeTabela = table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f3a8e0f-907e-4a25-aee7-793eac934db9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configurações do banco de dados em ODBC para dropar\n",
    "server_name = \"NOTEBOOK-LEO\\\\SQLEXPRESS\"\n",
    " \n",
    "# Conecte-se ao banco de dados usando pyodbc\n",
    "conn = pyodbc.connect(f\"Driver={{SQL Server}};Server={server_name};Database={database_name};UID={username};PWD={password}\")\n",
    "\n",
    "# Crie um cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute a operação DROP TABLE na tabela 'ProvasObjetivas'\n",
    "truncate_query = f\"IF OBJECT_ID('{table_name}', 'U') IS NOT NULL DROP TABLE {table_name}\"\n",
    "cursor.execute(truncate_query)\n",
    "\n",
    "# Commit as alterações no banco de dados\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e87ccad-a1e1-47ef-acc7-dfd6bfc677c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o356.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 22.0 failed 1 times, most recent failure: Lost task 50.0 in stage 22.0 (TID 281) (Notebook-Leo executor driver): com.microsoft.sqlserver.jdbc.SQLServerException: Não foi possível alocar espaço para o objeto 'dbo.ProvasObjetivas' do banco de dados 'Enem' porque o grupo de arquivos 'PRIMARY' está cheio. Crie espaço em disco excluindo arquivos desnecessários, removendo objetos do grupo de arquivos, adicionando arquivos ao grupo ou definido o aumento automático para arquivos existentes do grupo.\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:255)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1695)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatementBatch(SQLServerPreparedStatement.java:2978)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtBatchExecCmd.doExecute(SQLServerPreparedStatement.java:2826)\r\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7675)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4137)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:272)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:246)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2222)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:735)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1009)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1007)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:890)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:82)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: com.microsoft.sqlserver.jdbc.SQLServerException: Não foi possível alocar espaço para o objeto 'dbo.ProvasObjetivas' do banco de dados 'Enem' porque o grupo de arquivos 'PRIMARY' está cheio. Crie espaço em disco excluindo arquivos desnecessários, removendo objetos do grupo de arquivos, adicionando arquivos ao grupo ou definido o aumento automático para arquivos existentes do grupo.\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:255)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1695)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatementBatch(SQLServerPreparedStatement.java:2978)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtBatchExecCmd.doExecute(SQLServerPreparedStatement.java:2826)\r\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7675)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4137)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:272)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:246)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2222)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:735)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Escreva os novos dados em 'ProvasObjetivas' usando a conexão JDBC\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dfPreencher\u001b[39m.\u001b[39mwrite \\\n\u001b[0;32m      3\u001b[0m         \u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mjdbc\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m      4\u001b[0m         \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39murl\u001b[39m\u001b[39m\"\u001b[39m, url) \\\n\u001b[0;32m      5\u001b[0m         \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mdriver\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mcom.microsoft.sqlserver.jdbc.SQLServerDriver\u001b[39m\u001b[39m\"\u001b[39m)\\\n\u001b[0;32m      6\u001b[0m         \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mssl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m)\\\n\u001b[0;32m      7\u001b[0m         \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mtrustServerCertificate\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m)\\\n\u001b[0;32m      8\u001b[0m         \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mdbtable\u001b[39m\u001b[39m\"\u001b[39m, table_name) \\\n\u001b[0;32m      9\u001b[0m         \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, username) \\\n\u001b[0;32m     10\u001b[0m         \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mpassword\u001b[39m\u001b[39m\"\u001b[39m, password)\\\n\u001b[1;32m---> 11\u001b[0m         \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39moverwrite\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msave()\n",
      "File \u001b[1;32me:\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1396\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mformat\u001b[39m)\n\u001b[0;32m   1395\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1396\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[0;32m   1397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1398\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave(path)\n",
      "File \u001b[1;32me:\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32me:\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32me:\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o356.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 22.0 failed 1 times, most recent failure: Lost task 50.0 in stage 22.0 (TID 281) (Notebook-Leo executor driver): com.microsoft.sqlserver.jdbc.SQLServerException: Não foi possível alocar espaço para o objeto 'dbo.ProvasObjetivas' do banco de dados 'Enem' porque o grupo de arquivos 'PRIMARY' está cheio. Crie espaço em disco excluindo arquivos desnecessários, removendo objetos do grupo de arquivos, adicionando arquivos ao grupo ou definido o aumento automático para arquivos existentes do grupo.\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:255)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1695)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatementBatch(SQLServerPreparedStatement.java:2978)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtBatchExecCmd.doExecute(SQLServerPreparedStatement.java:2826)\r\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7675)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4137)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:272)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:246)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2222)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:735)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1009)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1007)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:890)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:82)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: com.microsoft.sqlserver.jdbc.SQLServerException: Não foi possível alocar espaço para o objeto 'dbo.ProvasObjetivas' do banco de dados 'Enem' porque o grupo de arquivos 'PRIMARY' está cheio. Crie espaço em disco excluindo arquivos desnecessários, removendo objetos do grupo de arquivos, adicionando arquivos ao grupo ou definido o aumento automático para arquivos existentes do grupo.\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:255)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1695)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatementBatch(SQLServerPreparedStatement.java:2978)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtBatchExecCmd.doExecute(SQLServerPreparedStatement.java:2826)\r\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7675)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:4137)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:272)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:246)\r\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeBatch(SQLServerPreparedStatement.java:2222)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:735)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# Escreva os novos dados em 'ProvasObjetivas' usando a conexão JDBC\n",
    "dfPreencher.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"driver\",\"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "        .option(\"ssl\", \"true\")\\\n",
    "        .option(\"trustServerCertificate\", \"true\")\\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", username) \\\n",
    "        .option(\"password\", password)\\\n",
    "        .option(\"mode\", \"overwrite\").save()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d07264e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe0615",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela ProvasObjetivas criada com sucesso no banco de dados.\n"
     ]
    }
   ],
   "source": [
    "print(f'Tabela {nomeTabela} criada com sucesso no banco de dados.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4ad95-23a6-45f1-af4d-d657f218bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d0b40d-2a48-40ba-b895-3ad862b5ce28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
