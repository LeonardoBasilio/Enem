{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31e718b-14d9-4751-9dec-71f56129a085",
   "metadata": {},
   "source": [
    "## Importação das Bibiliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc20c5a9-44d0-4ee3-ac2c-0f588099a1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "from pyspark.sql.functions import col, count, sum\n",
    "import pymssql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92050d4",
   "metadata": {},
   "source": [
    "## Criando uma SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137c301b-7b56-434b-bd08-979c6717af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.remote(\"sc://spark-connect\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ed9e6-77de-42fb-96ed-d1b23d0251a4",
   "metadata": {},
   "source": [
    "## Processo de Leitura do CSV de Participantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c848f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path dos arquivos\n",
    "\n",
    "caminho_arquivo_2022 = '/opt/datasets/ENEM/2022/microdados_enem_2022/DADOS/MICRODADOS_ENEM_2022.csv'\n",
    "caminho_arquivo_2021 = '/opt/datasets/ENEM/2021/microdados_enem_2021/DADOS/MICRODADOS_ENEM_2021.csv'\n",
    "caminho_arquivo_2020 = '/opt/datasets/ENEM/2020/microdados_enem_2020/DADOS/MICRODADOS_ENEM_2020.csv'\n",
    "caminho_arquivo_2019 = '/opt/datasets/ENEM/2019/microdados_enem_2019/DADOS/MICRODADOS_ENEM_2019.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65ad796f-3590-4234-b295-9ea745c3df22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/datasets/ENEM/2022/microdados_enem_2022/DADOS\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/opt/datasets/ENEM/2022/microdados_enem_2022/DADOS/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04836a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função Paa ler os arquivos CSV\n",
    "def lerCSV(spark, caminho_arquivo):\n",
    "    df = spark.read.csv(caminho_arquivo, sep=';', header=True, inferSchema=True, encoding='ISO-8859-1')\n",
    "    #colunas_selecionadas = df.columns[12:19]\n",
    "    #return df.select(colunas_selecionadas)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "158e76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Chama função quer ler o arquivo CSV e Transforma para DF.SPARK\n",
    "\n",
    "dfEscola22 = lerCSV(spark, caminho_arquivo_2022)\n",
    "#dfEscola21 = lerCSV(spark, caminho_arquivo_2021)\n",
    "#dfEscola20 = lerCSV(spark, caminho_arquivo_2020)\n",
    "#dfEscola19 = lerCSV(spark, caminho_arquivo_2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "579583cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unir os DF em um só \n",
    "\n",
    "#dfDadosGerais = dfEscola22.union(dfEscola21).union(dfEscola20).union(dfEscola19)\n",
    "dfDadosGerais = dfEscola22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0ddb3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[NU_INSCRICAO: bigint, NU_ANO: int, TP_FAIXA_ETARIA: int, TP_SEXO: string, TP_ESTADO_CIVIL: int, TP_COR_RACA: int, TP_NACIONALIDADE: int, TP_ST_CONCLUSAO: int, TP_ANO_CONCLUIU: int, TP_ESCOLA: int, TP_ENSINO: int, IN_TREINEIRO: int, CO_MUNICIPIO_ESC: int, NO_MUNICIPIO_ESC: string, CO_UF_ESC: int, SG_UF_ESC: string, TP_DEPENDENCIA_ADM_ESC: int, TP_LOCALIZACAO_ESC: int, TP_SIT_FUNC_ESC: int, CO_MUNICIPIO_PROVA: int, NO_MUNICIPIO_PROVA: string, CO_UF_PROVA: int, SG_UF_PROVA: string, TP_PRESENCA_CN: int, TP_PRESENCA_CH: int, TP_PRESENCA_LC: int, TP_PRESENCA_MT: int, CO_PROVA_CN: int, CO_PROVA_CH: int, CO_PROVA_LC: int, CO_PROVA_MT: int, NU_NOTA_CN: double, NU_NOTA_CH: double, NU_NOTA_LC: double, NU_NOTA_MT: double, TX_RESPOSTAS_CN: string, TX_RESPOSTAS_CH: string, TX_RESPOSTAS_LC: string, TX_RESPOSTAS_MT: string, TP_LINGUA: int, TX_GABARITO_CN: string, TX_GABARITO_CH: string, TX_GABARITO_LC: string, TX_GABARITO_MT: string, TP_STATUS_REDACAO: int, NU_NOTA_COMP1: int, NU_NOTA_COMP2: int, NU_NOTA_COMP3: int, NU_NOTA_COMP4: int, NU_NOTA_COMP5: int, NU_NOTA_REDACAO: int, Q001: string, Q002: string, Q003: string, Q004: string, Q005: int, Q006: string, Q007: string, Q008: string, Q009: string, Q010: string, Q011: string, Q012: string, Q013: string, Q014: string, Q015: string, Q016: string, Q017: string, Q018: string, Q019: string, Q020: string, Q021: string, Q022: string, Q023: string, Q024: string, Q025: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPreencher = dfDadosGerais.fillna(-2)  # Preenche com -2(sem ref), por exemplo\n",
    "dfPreencher.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b839ff03-3177-4106-9812-c704694c2b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[NU_INSCRICAO: bigint, ANO_EDICAO: int, ANO_CONCLUSAO: int, SEXO_CODIGO: string, MUNICIPIO_CODIGO: int, ESTADO_CIVIL_ID: int, COR_RACA_ID: int, NACIONALIDADE_ID: int, SITUACAO_ESCOLARIDADE_ID: int, ESCOLA_ID: int, FAIXA_RENDA_MENSAL_ID: string, FAIXA_ETARIA_ID: int, NOTA_CIENCIA_DA_NATUREZA: double, NOTA_CIENCIA_DA_HUMANA: double, NOTA_LINGUAGEM_CODIGO: double, NOTA_MATEMATICA: double]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPreencherDistinct = dfPreencher.select(\n",
    "    col(\"NU_INSCRICAO\").alias(\"NU_INSCRICAO\"),\n",
    "    col(\"NU_ANO\").alias(\"ANO_EDICAO\"),\n",
    "    col(\"TP_ANO_CONCLUIU\").alias(\"ANO_CONCLUSAO\"),\n",
    "    col(\"TP_SEXO\").alias(\"SEXO_CODIGO\"),\n",
    "    col(\"CO_MUNICIPIO_ESC\").alias(\"MUNICIPIO_CODIGO\"),\n",
    "    col(\"TP_ESTADO_CIVIL\").alias(\"ESTADO_CIVIL_ID\"),\n",
    "    col(\"TP_COR_RACA\").alias(\"COR_RACA_ID\"),\n",
    "    col(\"TP_NACIONALIDADE\").alias(\"NACIONALIDADE_ID\"),\n",
    "    col(\"TP_ST_CONCLUSAO\").alias(\"SITUACAO_ESCOLARIDADE_ID\"),\n",
    "    col(\"TP_ESCOLA\").alias(\"ESCOLA_ID\"),\n",
    "    col(\"Q006\").alias(\"FAIXA_RENDA_MENSAL_ID\"),\n",
    "    col(\"TP_FAIXA_ETARIA\").alias(\"FAIXA_ETARIA_ID\"),\n",
    "    col(\"NU_NOTA_CN\").alias(\"NOTA_CIENCIA_DA_NATUREZA\"),\n",
    "    col(\"NU_NOTA_CH\").alias(\"NOTA_CIENCIA_DA_HUMANA\"),\n",
    "    col(\"NU_NOTA_LC\").alias(\"NOTA_LINGUAGEM_CODIGO\"),\n",
    "    col(\"NU_NOTA_MT\").alias(\"NOTA_MATEMATICA\")\n",
    ").distinct()\n",
    "dfPreencherDistinct.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a8852d0-721d-41a7-a6f9-366cd5d2c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAgregate = dfPreencherDistinct.select(\n",
    "    \"ANO_EDICAO\",\n",
    "    \"ANO_CONCLUSAO\",\n",
    "    \"SEXO_CODIGO\",\n",
    "    \"MUNICIPIO_CODIGO\",\n",
    "    \"ESTADO_CIVIL_ID\",\n",
    "    \"COR_RACA_ID\",\n",
    "    \"NACIONALIDADE_ID\",\n",
    "    \"SITUACAO_ESCOLARIDADE_ID\",\n",
    "    \"ESCOLA_ID\",\n",
    "    \"FAIXA_RENDA_MENSAL_ID\",\n",
    "    \"FAIXA_ETARIA_ID\",\n",
    "    \"NU_INSCRICAO\",  # Adicionando essa coluna para o count\n",
    "    \"NOTA_CIENCIA_DA_NATUREZA\",\n",
    "    \"NOTA_CIENCIA_DA_HUMANA\",\n",
    "    \"NOTA_LINGUAGEM_CODIGO\",\n",
    "    \"NOTA_MATEMATICA\"\n",
    ")\n",
    "\n",
    "# Agregação após o agrupamento\n",
    "dfAgregate = dfAgregate.groupBy(\n",
    "    \"ANO_EDICAO\",\n",
    "    \"ANO_CONCLUSAO\",\n",
    "    \"SEXO_CODIGO\",\n",
    "    \"MUNICIPIO_CODIGO\",\n",
    "    \"ESTADO_CIVIL_ID\",\n",
    "    \"COR_RACA_ID\",\n",
    "    \"NACIONALIDADE_ID\",\n",
    "    \"SITUACAO_ESCOLARIDADE_ID\",\n",
    "    \"ESCOLA_ID\",\n",
    "    \"FAIXA_RENDA_MENSAL_ID\",\n",
    "    \"FAIXA_ETARIA_ID\"\n",
    ").agg(\n",
    "    count(\"NU_INSCRICAO\").alias(\"QTD\"),\n",
    "    sum(\"NOTA_CIENCIA_DA_NATUREZA\").alias(\"NOTA_CIENCIA_DA_NATUREZA\"),\n",
    "    sum(\"NOTA_CIENCIA_DA_HUMANA\").alias(\"NOTA_CIENCIA_DA_HUMANA\"),\n",
    "    sum(\"NOTA_LINGUAGEM_CODIGO\").alias(\"NOTA_LINGUAGEM_CODIGO\"),\n",
    "    sum(\"NOTA_MATEMATICA\").alias(\"NOTA_MATEMATICA\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9fbdb3-fa7c-4592-8d43-42d0553f75e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAgregate.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ed5ca",
   "metadata": {},
   "source": [
    "# Salvar em SQL \n",
    "Salvar os dados em uma tabela SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1deb1f",
   "metadata": {},
   "source": [
    "## Parametrizando a conexão do Banco SQL SERVER Usando JDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e7396db",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_name = \"46aa1e5a9d50\"\n",
    "database_name = \"DockerEnem\"\n",
    "username = \"sa\"\n",
    "password = \"bi@123456\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3234ec7-5988-465d-83b4-e623266a33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome da tabela a ser excluída\n",
    "table_name = \"ODS_ENEM\"\n",
    "\n",
    "#obter Schema do dfAgregate \n",
    "df_schema = dfAgregate.schema\n",
    "\n",
    "# Gerando as definições de coluna para a tabela\n",
    "columns_definitions = [f\"{field.name} {field.dataType}\" for field in df_schema.fields]\n",
    "\n",
    "# Mapeamento de tipos de dados do Spark para tipos de dados do SQL Server\n",
    "data_type_mapping = {\n",
    "    \"IntegerType\": \"INT\",\n",
    "    \"StringType\": \"NVARCHAR(255)\",  # Você pode ajustar o tamanho conforme necessário\n",
    "    \"DoubleType\": \"FLOAT\",\n",
    "    \"LongType\": \"BIGINT\"\n",
    "}\n",
    "\n",
    "# Gerando as definições de coluna para a tabela com os tipos de dados corretos\n",
    "columns_definitions = [f\"{field.name} {data_type_mapping.get(str(field.dataType), 'NVARCHAR(255)')}\" for field in df_schema.fields]\n",
    "\n",
    "\n",
    "# Combinando as definições de coluna em uma string\n",
    "columns_str = \",\\n\".join(columns_definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45262d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conectando ao banco de dados\n",
    "conn = pymssql.connect(server=server_name, database=database_name, user=username, password=password)\n",
    "\n",
    "# Criando um cursor\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f3a8e0f-907e-4a25-aee7-793eac934db9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SparkConnectGrpcException",
     "evalue": "(org.apache.spark.SparkException) Job aborted due to stage failure: ResultStage 75 (executePlan at SparkConnectServiceGrpc.java:449) has failed the maximum allowable number of times: 4. Most recent failure reason:\norg.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 3 partition 171\n\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)\n\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)\n\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)\n\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(Re...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSparkConnectGrpcException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m cursor\u001b[38;5;241m.\u001b[39mexecute(create_table_query)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Inserindo os dados na tabela\u001b[39;00m\n\u001b[1;32m     45\u001b[0m insert_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;124mINSERT INTO ODS_ENEM VALUES \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39mrow))\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mrow\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[43mdfAgregate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m])\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     48\u001b[0m cursor\u001b[38;5;241m.\u001b[39mexecute(insert_query)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Commit das alterações no banco de dados\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:1354\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot collect on empty session.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1353\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mto_proto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n\u001b[0;32m-> 1354\u001b[0m table, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m schema \u001b[38;5;241m=\u001b[39m schema \u001b[38;5;129;01mor\u001b[39;00m from_arrow_schema(table\u001b[38;5;241m.\u001b[39mschema)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, StructType)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/connect/client.py:668\u001b[0m, in \u001b[0;36mSparkConnectClient.to_table\u001b[0;34m(self, plan)\u001b[0m\n\u001b[1;32m    666\u001b[0m req \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_plan_request_with_metadata()\n\u001b[1;32m    667\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mCopyFrom(plan)\n\u001b[0;32m--> 668\u001b[0m table, schema, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table, schema\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/connect/client.py:982\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m    979\u001b[0m schema: Optional[StructType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    980\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(req):\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n\u001b[1;32m    984\u001b[0m         schema \u001b[38;5;241m=\u001b[39m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/connect/client.py:963\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m    961\u001b[0m                             \u001b[38;5;28;01myield\u001b[39;00m batch\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/connect/client.py:1055\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;124;03mHandle errors that occur during RPC calls.\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;124;03mThrows the appropriate internal Python exception.\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[0;32m-> 1055\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/connect/client.py:1091\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             info \u001b[38;5;241m=\u001b[39m error_details_pb2\u001b[38;5;241m.\u001b[39mErrorInfo()\n\u001b[1;32m   1090\u001b[0m             d\u001b[38;5;241m.\u001b[39mUnpack(info)\n\u001b[0;32m-> 1091\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(info, status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mSparkConnectGrpcException\u001b[0m: (org.apache.spark.SparkException) Job aborted due to stage failure: ResultStage 75 (executePlan at SparkConnectServiceGrpc.java:449) has failed the maximum allowable number of times: 4. Most recent failure reason:\norg.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 3 partition 171\n\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1718)\n\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1665)\n\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1664)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1664)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1306)\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1268)\n\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:140)\n\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:63)\n\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:57)\n\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(Re..."
     ]
    }
   ],
   "source": [
    "# Dropando a tabela se ela já existir\n",
    "drop_table_query = \"\"\"\n",
    "IF OBJECT_ID('ODS_ENEM', 'U') IS NOT NULL\n",
    "    DROP TABLE ODS_ENEM\n",
    "\"\"\"\n",
    "cursor.execute(drop_table_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02afb5-f05e-4c46-aa3c-c612bdef245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o comando CREATE TABLE\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE {table_name} (\n",
    "{columns_str}\n",
    ")\n",
    "\"\"\"\n",
    "# Executando o comando SQL para criar a tabela\n",
    "cursor.execute(create_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3e70f-47d3-406b-8f08-506d14b0685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserindo os dados na tabela\n",
    "insert_query = f\"\"\"\n",
    "INSERT INTO ODS_ENEM VALUES {\",\".join([\"(\" + \",\".join(map(str, row)) + \")\" for row in dfAgregate.collect()])}\n",
    "\"\"\"\n",
    "cursor.execute(insert_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26097f3-92df-4b68-b475-12a67edc300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commit das alterações no banco de dados\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871271ef-ac95-4286-bb72-88e1e23fd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fechando a conexão\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe0615",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Tabela {nomeTabela} criada com sucesso no banco de dados.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
